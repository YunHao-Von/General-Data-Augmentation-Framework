## 1.Introduction  
Instead of feeding the input latent code z ∈ Z only to the beginning of a the network, the mapping network f first transforms it to an intermediate latent code w ∈ W. Affine transforms then produce styles that control the layers of the synthesis network gvia adaptive instance normalization (AdaIN).Additionally, stochastic variation is facilitated by providing additional random noise maps to the synthesis network.  
映射网络 f 不是仅将输入潜码 z ∈ Z 输送到网络的起始位置，同时还首先将其转换为中间潜码 w ∈ W。然后仿射变换通过自适应实例归一化生成控制合成网络g的样式I(AdaIN)。此外，通过向合成网络提供额外的随机噪声图来促进随机变化。  

First, we investigate the origin of common blob-like artifacts, and find that the generator creates them to circumvent a design flaw in its architecture. In Section 2, we redesign the normalization used in the generator, which removes the artifacts. Second, we analyze artifacts related to progressive growing [23] that has been highly successful in stabilizing high-resolution GAN training. We propose an alternative design that achieves the same goal — training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions — without changing the network topology during training. This new design also allows us to reason about the effective resolution of the generated images, which turns out to be lower than expected, motivating a capacity increase (Section 4).  
首先，我们调查常见的斑点状人工产物的起源，并发现生成器创建它们是为了绕过其体系结构中的一个设计缺陷。在第2节中，我们重新设计了在生成器中使用的规归一化模块，它删除了artifacts。其次，我们分析了与渐进生长相关的artifacts[23]，这些artifacts在稳定高分辨率GAN训练方面非常成功。我们提出了一种实现相同目标的替代设计--训练从低分辨率图像开始，然后逐渐将焦点转移到越来越高的分辨率--在训练过程中不改变网络拓扑。这种新的设计还允许我们对生成的图像的有效分辨率进行推理，结果发现分辨率低于预期，从而推动了容量的增加(第4节)。  
